{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b80b34-023a-4c07-aa9a-187989972a84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on cuda:0 device\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=5, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=5, bias=True)\n",
       "          (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-255): 256 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=5, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=5, bias=True)\n",
       "          (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = {'FECHA': pd.date_range(start='1/1/2020', periods=100),\n",
    "        'DIA': np.random.randint(1, 32, 100),\n",
    "        'MES': np.random.randint(1, 13, 100),\n",
    "        'ANO': np.full(100, 2020),\n",
    "        'SEMANA': np.random.randint(1, 53, 100),\n",
    "        'DIASEMANA': np.random.randint(1, 8, 100),\n",
    "        'CANTIDAD': np.random.randint(1, 100, 100)}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalizar las columnas numéricas utilizando MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_values = scaler.fit_transform(df.drop(columns=['FECHA', 'CANTIDAD']))\n",
    "\n",
    "# Crear las secuencias de tamaño fijo (por ejemplo, longitud 5)\n",
    "sequence_length = 5\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(scaled_values[i:i+sequence_length])\n",
    "    y.append(df['CANTIDAD'].iloc[i+sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Crear un conjunto de datos personalizado para PyTorch\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = TimeSeriesDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, dim_feedforward)\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Crear una máscara de atención triangular inferior para asegurar que las posiciones futuras no afecten las predicciones\n",
    "        attn_mask = torch.triu(torch.ones(x.size(0), x.size(0)), diagonal=1).bool().to(x.device)\n",
    "        \n",
    "        # Proporcionar el mismo tensor para las entradas de origen y destino\n",
    "        x = self.transformer(x, x,tgt_mask=attn_mask)\n",
    "        x = self.linear(x[:, -1])\n",
    "        return x\n",
    "\n",
    "# Hiperparámetros del modelo\n",
    "d_model = 5  # Cambiar esto para que coincida con la cantidad de características en tus datos\n",
    "nhead = 5\n",
    "num_layers = 3\n",
    "dim_feedforward = 256\n",
    "\n",
    "# Crear el modelo\n",
    "model = CustomTransformer(d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "# Define your execution device \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(\"The model will be running on\", device, \"device\\n\") \n",
    "model.to(device)    # Convert model parameters and buffers to CPU or Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa29498-1c2a-4a2c-8dca-6c1222db2763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Entrenar el modelo con tus datos (necesitarás ajustar esto a tu problema específico)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        # Asegurar que los datos tienen la forma correcta (S, N, E)\n",
    "        # batch_X = batch_X.transpose(0, 1)\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        # Hacer predicciones con los datos de entrada\n",
    "        predictions = model(batch_X).squeeze(-1)\n",
    "        \n",
    "        # Calcular la pérdida\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        \n",
    "        # Optimizar el modelo\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3197199f-3464-484c-b38a-bef13614f379",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1627.7230, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5753d65-6cf2-4f18-828e-044ed15bbda8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48aeb0-d2da-4be5-939d-2b2efc549d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
